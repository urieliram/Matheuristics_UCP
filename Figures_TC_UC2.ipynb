{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAY/1gzeb7MCLA5TbUbOK+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urieliram/tc_uc/blob/main/Figures_TC_UC2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paquetes"
      ],
      "metadata": {
        "id": "6G9_Mw2F4fHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "grzyvyw38-UX"
      },
      "outputs": [],
      "source": [
        "# load packages\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re\n",
        "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
        "\n",
        "# Gráficos\n",
        "# ==============================================================================\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "cm = 1/2.54\n",
        "\n",
        "# Modelado y test estadísticos\n",
        "# ============================================================================\n",
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "from scipy.stats import ks_2samp\n",
        "from scipy import stats\n",
        "from scipy.stats import shapiro\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Configuración matplotlib\n",
        "# ==============================================================================\n",
        "# Gráficos distribución observada \n",
        "# ============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# plt.style.use('fivethirtyeight')\n",
        "import seaborn as sns\n",
        "# plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones"
      ],
      "metadata": {
        "id": "dQjCbDCIbVK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trunc(values, decs=1):\n",
        "    return np.trunc(values*10**decs)/(10**decs)"
      ],
      "metadata": {
        "id": "LTCmvPoh3-Lh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gap(bestbound,bestinteger):\n",
        "  return abs(bestbound-bestinteger)/(1e-10+abs(bestinteger)) ## https://www.ibm.com/docs/pl/icos/12.9.0?topic=parameters-relative-mip-gap-tolerance"
      ],
      "metadata": {
        "id": "pUQzwifVWOfQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outliers(data,max_deviations):\n",
        "  mean    = np.mean(data)\n",
        "  std_dev = np.std( data)  \n",
        "  zero_based = abs(data - mean)   ## Normalize array around 0\n",
        "  data = data[zero_based < max_deviations * std_dev] ## Access only non-outliers using Boolean Indexing\n",
        "  # plt.hist(data, alpha=0.75)\n",
        "  # plt.show()\n",
        "  return data"
      ],
      "metadata": {
        "id": "fTtAoC4C1fGo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outliers_intervals(data,max_deviations):\n",
        "  mean    = np.mean(data)\n",
        "  std_dev = np.std( data)  \n",
        "  zero_based = abs(data - mean)   ## Normalize array around 0\n",
        "  data = data[zero_based < max_deviations * std_dev] ## Access only non-outliers using Boolean Indexing\n",
        "  # plt.hist(data, alpha=0.75)\n",
        "  # plt.show()\n",
        "  return mean-2*std_dev,mean+2*std_dev"
      ],
      "metadata": {
        "id": "58sE8E6s7ms9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pie(title='Apple pie',labels=['Apple','Cinammon'],sizes=[7,3],colors=['#1f77b4', '#ff7f0e','#66b3ff','#ff6666','#99ff99',],label=''):\n",
        "  explode = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
        "  fig1, ax1 = plt.subplots()\n",
        "  mpl.rcParams['font.size'] = fontsize\n",
        "  ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
        "          shadow=True, colors=colors, startangle=0)\n",
        "  ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "  ax1.set_title(title)\n",
        "  fig.tight_layout()\n",
        "  namefile = dir+'fig:pie'+'_'+title+'.pdf'\n",
        "  plt.savefig(namefile, transparent=True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "4CFKDfs60kiR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fontsize=14\n",
        "palette=['#1f77b4','#ff7f0e','#2ca02c','#d62728','#9467bd','#8c564b','#e377c2','#7f7f7f','#bcbd22','#17becf']\n",
        "# tab:blue,tab:orange,tab:green,tab:red,tab:purple,coral,tab:brown,tab:pink,tab:gray,tab:olive,tab:cyan\n",
        "# my_pal = {\"versicolor\": \"g\", \"setosa\": \"b\", \"virginica\": \"m\"}\n",
        " \n",
        "# # plot it\n",
        "# sns.violinplot(x=df[\"species\"], y=df[\"sepal_length\"], palette=my_pal)"
      ],
      "metadata": {
        "id": "o8oQmxtrGRXL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRÁFICOS DE CONVERGENCIA milp, milp2, hard3, harjk lbc, ks, \n"
      ],
      "metadata": {
        "id": "MdVaYxkv5LNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura de archivos log"
      ],
      "metadata": {
        "id": "rJMRbTfUqPRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Lectura de datos\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "_VBW1elDnJTD",
        "outputId": "a2ab5718-bb6c-4724-a34a-5032062452b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5ef70727-b350-48fa-b443-592d51a163ea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5ef70727-b350-48fa-b443-592d51a163ea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"Log1.zip\" -d \"/content\"\n",
        "#         ó\n",
        "# !pip install unrar\n",
        "# !unrar x \"/content/Logs1.rar\""
      ],
      "metadata": {
        "id": "WLmdjGzdp6ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura de archivo de resultados stat.xls"
      ],
      "metadata": {
        "id": "OlTVywqOCDV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/drive/folders/1Gj3XK9kM-lE18uBMe3qrZOGEm8yAI8i9\n",
        "#https://www.codegrepper.com/code-examples/python/how+to+read+csv+file+from+google+drive+on+google+colab+\n",
        "path = 'https://drive.google.com/uc?export=download&id=' \n",
        "# URL  = 'https://drive.google.com/file/d/1ySF5y3FdEZrEAu1jZ_P1PdBJSk64TQvt/view?usp=share_link'\n",
        "# df   = pd.read_csv(path+URL.split('/')[-2],header=0)\n",
        "URL        = 'https://docs.google.com/spreadsheets/d/12byid7r6dwnpYgbWWrvWrYQtlFfsPhI6Meq9tcF-DBU/edit?usp=sharing'\n",
        "sheet_id   = '12byid7r6dwnpYgbWWrvWrYQtlFfsPhI6Meq9tcF-DBU'\n",
        "sheet_name = 'stat1'\n",
        "url        = f'https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}'\n",
        "df         = pd.read_csv(url,header=0)\n",
        "dfgraph    = pd.read_csv(url,header=0)\n",
        "dfgraph    = dfgraph.loc[(dfgraph['groupx']=='x7day_small')|(dfgraph['groupx']=='x7day_medium')|(dfgraph['groupx']=='x7day_large')] \n",
        "# df.dropna(inplace=True)\n",
        "## x10gen_small 081-090\n",
        "## x10gen_large 091-100\n",
        "## x7day_small  061-070\n",
        "## x7day_large  071-080 , 131-152\n",
        "## x7day_medium 101-131\n",
        "\n",
        "dfx = df[(df['groupx'] == 'x7day_small' )]\n",
        "dfy = df[(df['groupx'] == 'x7day_medium')]\n",
        "dfz = df[(df['groupx'] == 'x7day_large' )]\n",
        "filter = 'x7day_medium' ## x10gen_small, x10gen_large ,x7day_small, x7day_large, \n",
        "df = df[(df['groupx'] == filter)]\n",
        "\n",
        "!mkdir Figures\n",
        "dir = '/content/Figures/'"
      ],
      "metadata": {
        "id": "VPYCprJlrHYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "for ins in dfx.nameins:\n",
        "  labels.append(ins[0:6])\n",
        "for ins in dfy.nameins:\n",
        "  labels.append(ins[0:6])\n",
        "for ins in dfz.nameins:\n",
        "  labels.append(ins[0:6])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "alpha=0.7\n",
        "ax.plot(range(0         , len(dfx.G)),                                  dfx.G, 'o', label = 'x7day_small', markersize=10,alpha=alpha)\n",
        "ax.plot(range(len(dfx.G), len(dfx.G)+len(dfy.G)),                       dfy.G, 'o', label = 'x7day_medium',markersize=10,alpha=alpha)\n",
        "ax.plot(range(len(dfx.G)+len(dfy.G), len(dfx.G)+len(dfy.G)+len(dfz.G)), dfz.G, 'o', label = 'x7day_large', markersize=10,alpha=alpha)\n",
        "index=range(0 ,len(dfx.G)+len(dfy.G)+len(dfz.G))\n",
        "plt.xticks(index, labels, rotation='vertical', fontsize=9,)\n",
        "plt.yticks( fontsize=12)\n",
        "plt.xlabel(\"Instances\", fontsize=14)\n",
        "plt.ylabel(\"Number of generators\", fontsize=14)\n",
        "plt.legend(fontsize=13,loc='upper center')\n",
        "\n",
        "ax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "plt.ylim([0,1E-3])\n",
        "\n",
        "alpha=1\n",
        "ax2.plot(range(0              , len(dfx.g_milp)),                                                 dfx.g_milp,'x', markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_milp), len(dfx.g_milp)+len(dfy.g_milp)),                                 dfy.g_milp,'x',markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_milp)+len(dfy.g_milp), len(dfx.g_milp)+len(dfy.g_milp)+len(dfz.g_milp)), dfz.g_milp,'x', markersize=10,alpha=alpha)\n",
        "\n",
        "alpha=0\n",
        "ax2.plot(range(0               , len(dfx.g_milp2)),                                                    dfx.g_milp2,'x', markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_milp2), len(dfx.g_milp2)+len(dfy.g_milp2)),                                   dfy.g_milp2,'x',markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_milp2)+len(dfy.g_milp2), len(dfx.g_milp2)+len(dfy.g_milp2)+len(dfz.g_milp2)), dfz.g_milp2,'x', markersize=10,alpha=alpha)\n",
        "\n",
        "alpha=0.3\n",
        "ax2.plot(range(0              , len(dfx.g_lbc1)),                                                 dfx.g_lbc1,'.', markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_lbc1), len(dfx.g_lbc1)+len(dfy.g_lbc1)),                                 dfy.g_lbc1,'.',markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_lbc1)+len(dfy.g_lbc1), len(dfx.g_lbc1)+len(dfy.g_lbc1)+len(dfz.g_lbc1)), dfz.g_lbc1,'.', markersize=10,alpha=alpha)\n",
        "\n",
        "ax2.plot(range(0              , len(dfx.g_lbc2)),                                                 dfx.g_lbc2,'.', markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_lbc2), len(dfx.g_lbc2)+len(dfy.g_lbc2)),                                 dfy.g_lbc2,'.',markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_lbc2)+len(dfy.g_lbc2), len(dfx.g_lbc2)+len(dfy.g_lbc2)+len(dfz.g_lbc2)), dfz.g_lbc2,'.', markersize=10,alpha=alpha)\n",
        "\n",
        "ax2.plot(range(0              , len(dfx.g_lbc3)),                                                 dfx.g_lbc3,'.', markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_lbc3), len(dfx.g_lbc3)+len(dfy.g_lbc3)),                                 dfy.g_lbc3,'.',markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_lbc3)+len(dfy.g_lbc3), len(dfx.g_lbc3)+len(dfy.g_lbc3)+len(dfz.g_lbc3)), dfz.g_lbc3,'.', markersize=10,alpha=alpha)\n",
        "\n",
        "ax2.plot(range(0              , len(dfx.g_lbc4)),                                                 dfx.g_lbc4,'.', markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_lbc4), len(dfx.g_lbc4)+len(dfy.g_lbc4)),                                 dfy.g_lbc4,'.',markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_lbc4)+len(dfy.g_lbc4), len(dfx.g_lbc4)+len(dfy.g_lbc4)+len(dfz.g_lbc4)), dfz.g_lbc4,'.', markersize=10,alpha=alpha)\n",
        "\n",
        "ax2.plot(range(0            , len(dfx.g_ks)),                                           dfx.g_ks,'.',markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_ks), len(dfx.g_ks)+len(dfy.g_ks)),                             dfy.g_ks,'.',markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_ks)+len(dfy.g_ks), len(dfx.g_ks)+len(dfy.g_ks)+len(dfz.g_ks)), dfz.g_ks,'.',markersize=10,alpha=alpha)\n",
        "\n",
        "alpha = 0\n",
        "ax2.plot(range(0               , len(dfx.g_harjk)),                                                    dfx.g_harjk,'.', markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_harjk), len(dfx.g_harjk)+len(dfy.g_harjk)),                                   dfy.g_harjk,'.',markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_harjk)+len(dfy.g_harjk), len(dfx.g_harjk)+len(dfy.g_harjk)+len(dfz.g_harjk)), dfz.g_harjk,'.', markersize=10,alpha=alpha)\n",
        "\n",
        "alpha = 0\n",
        "ax2.plot(range(0                , len(dfx.g_harduc)),                                                       dfx.g_harduc,'*', markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_harduc), len(dfx.g_harduc)+len(dfy.g_harduc)),                                     dfy.g_harduc,'*',markersize=10,alpha=alpha)\n",
        "ax2.plot(range(len(dfx.g_harduc)+len(dfy.g_harduc), len(dfx.g_harduc)+len(dfy.g_harduc)+len(dfz.g_harduc)), dfz.g_harduc,'*', markersize=10,alpha=alpha)\n",
        "\n",
        "\n",
        "plt.ylabel(\"% gap\", fontsize=14)\n",
        "# plt.title('Instances'+'\\n ('+filter+')') ###########################################################################\n",
        "#plt.legend(bbox_to_anchor=(1, 0.75))\n",
        "plt.tight_layout()\n",
        "plt.savefig(dir+'fig:'+'instances.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sUzMwM-Hyt_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## https://seaborn.pydata.org/generated/seaborn.jointplot.html\n",
        "sns.jointplot(data=dfgraph, x=dfgraph.G, y=dfgraph.g_milp,hue=dfgraph.groupx)"
      ],
      "metadata": {
        "id": "QOX4mBUhg0Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list=[]\n",
        "nameins   = df.nameins\n",
        "for i in nameins:\n",
        "  list.append('Log_'+i.replace('.json', '').replace('uc_', ''))\n",
        "print('n=',len(list),'instances')\n",
        "n=len(list)"
      ],
      "metadata": {
        "id": "1KIbQZubFWTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LOWER BOUND MILP\n",
        "len(df.lb_milp)\n",
        "lb_milp\t= pd.to_numeric(df.lb_milp).to_numpy() "
      ],
      "metadata": {
        "id": "y0R5Mmf6UVHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dfgraph.t_harduc)\n",
        "t_harjk\t= pd.to_numeric(df.t_harjk).to_numpy()\n",
        "len(df.t_harduc)\n",
        "t_harduc\t= pd.to_numeric(df.t_harduc).to_numpy()\n",
        "plt.figure()\n",
        "plt.plot(t_harjk)\n",
        "plt.plot(t_harduc)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WdCNh7x-Nhpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extrae información de los logs y construye gráficas"
      ],
      "metadata": {
        "id": "m7bbxKz9qYKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtractCPLEX:\n",
        "\n",
        "  def findPos(self,file,spected, starter = \"Node  Left\"):\n",
        "    file = open(file,\"r\")\n",
        "    for i,j in enumerate(file):\n",
        "      if starter in j:\n",
        "        return [(j.index(k)+len(k)-1) for k in spected]\n",
        "  \n",
        "  def getFilelog(self,f):\n",
        "    with open(f, 'r') as file:\n",
        "      data = file.read().replace('\\n', '')\n",
        "    tmp = []\n",
        "    for i in re.findall(r\"\\w*Logfile\\s*\\'*[a-zA-z.+]*\\'\",data):\n",
        "      tmp.append(i.replace(\"'\",\"\").split(\" \")[1])\n",
        "    return tmp\n",
        "\n",
        "  def createTables(self,fn):\n",
        "    table_start = False\n",
        "    spected = ['Node', 'Left', 'Objective', 'IInf', 'Integer', 'Bound', 'ItCnt', 'Gap']\n",
        "    expectedPositions = self.findPos(fn,spected)\n",
        "    tables = {\"seconds\":[],\"ticks\":[],\"solution\":[],\"Node\":[],\"Left\":[],\"Objective\":[],\"IInf\":[],\"Integer\":[],\"Bound\":[],\"ItCnt\":[],\"Gap\":[],\"cuts\":[]}\n",
        "    time = None\n",
        "    ticks = None\n",
        "    cuts = None\n",
        "    f = open(fn,\"r\")    \n",
        "    for i in f:\n",
        "      if re.findall(r\"\\d{1,}[+]{1}\\d{1,}\",i):\n",
        "        i = \" \".join(i.split(\"+\"))\n",
        "      if(\"Cover cuts applied\" in i or \"Performing restart 1\" in i):\n",
        "        table_start = False\n",
        "      if(\"Elapsed time\" in i):\n",
        "          tmp = [float(k) for k in re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)]\n",
        "          time = tmp[0]\n",
        "          ticks = tmp[1]\n",
        "      if(table_start):        \n",
        "        if((str(i)[0] == \" \" or str(i)[0] == \"*\") and str(i)[expectedPositions[0]].isdigit()):\n",
        "          i = i.replace(\"uts: \",\" uts:\")\n",
        "          tables[\"seconds\"].append(time)\n",
        "          tables[\"ticks\"].append(ticks)\n",
        "          tables[\"solution\"].append(1 if i[0]==\"*\" else 0)\n",
        "          tables[\"cuts\"].append(None)\n",
        "          for j,m in zip(expectedPositions,spected):\n",
        "            if i[j] != \" \":\n",
        "              tmp = \"\"\n",
        "              for k in range(j,0,-1):\n",
        "                if i[k] != \" \":\n",
        "                  tmp += i[k]\n",
        "                else: \n",
        "                  break \n",
        "              tmp = tmp[::-1]\n",
        "              if(\"ut\" in tmp.lower() or \"infeasible\" in tmp.lower() or \"integral\" in tmp.lower()):\n",
        "                if(\"uts\" in tmp.lower()):\n",
        "                  tables[\"cuts\"][-1] = int(tmp.split(\":\")[1])\n",
        "                tables[m].append(tables[m][-1])\n",
        "              else:\n",
        "                if m == \"Gap\":\n",
        "                  tables[m].append(float(tmp)/100)\n",
        "                else: \n",
        "                  tables[m].append(float(tmp))\n",
        "            else:\n",
        "              tables[m].append(None)\n",
        "      if(\"Node  Left\"):\n",
        "        table_start = True\n",
        "    return tables\n",
        "\n",
        "  def extract(self,fn):\n",
        "    variables = {\"mipPresolveEliminated\":[],\"mipPresolveModified\":[],\"aggregatorDid\":[],\"reducedMipHasColumns\":[],\"reducedMipHasNonZero\":[],\"reducedMipHasBinaries\":[],\"reducedMipHasGeneral\":[],\"cliqueTableMembers\":[],\"rootRelaxSolSeconds\":[],\"rootRelaxSolTicks\":[]}\n",
        "    variables[\"logFile\"] = self.getFilelog(fn)\n",
        "    tables = self.createTables(fn)\n",
        "    f = open(fn, \"r\")    \n",
        "    for i in f:\n",
        "      if(\"linear optimization\" in i):\n",
        "        variables[\"linearOpt\"] = float(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1])\n",
        "      if(\"optimality gap tolerance\" in i):\n",
        "        variables[\"gapTol\"] = float(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1])\n",
        "      if(\"time limit in seconds\" in i):\n",
        "        variables[\"timeLimit\"] = float(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1])\n",
        "      if(\"emphasis for MIP optimization\" in i):\n",
        "        variables[\"mipOpt\"] = float(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1])\n",
        "      if(\"Objective sense\" in i):\n",
        "        variables[\"objSense\"] = i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1]\n",
        "      if(\"Variables\" in i):\n",
        "        if(\"Box:\" in i):\n",
        "          variablesValue = [\"variablesValue\",\"Nneg\",\"Box\",\"Binary\"]\n",
        "          for j,k in enumerate(re.findall(r'\\d+', i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\",1)[1])):\n",
        "            variables[variablesValue[j]] = float(k)\n",
        "        else:\n",
        "          variablesValue = [\"minLB\",\"maxUb\"]\n",
        "          for j,k in enumerate(re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)):\n",
        "            variables[variablesValue[j]] = float(k)\n",
        "      if(\"Objective nonzeros\" in i):\n",
        "        if(\"Min\" in i or \"Max\" in i):\n",
        "          variablesValue = [\"objNonZerosMin\",\"objNonZerosMax\"]\n",
        "          for j,k in enumerate(re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)):\n",
        "            variables[variablesValue[j]] = float(k)\n",
        "        else:\n",
        "          variables[\"objNonZeros\"] = float(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1])\n",
        "      if(\"Linear constraints\" in i):\n",
        "        if(\"Less\" in i):\n",
        "          variablesValue = [\"linearConstraintsValue\",\"less\",\"greater\",\"equal\"]\n",
        "          for j,k in enumerate(re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)):\n",
        "            variables[variablesValue[j]] = float(k)\n",
        "        else:\n",
        "          pass\n",
        "      if(\"Nonzeros\" in i):\n",
        "        if(\"Min\" in i):\n",
        "          variablesValue = [\"nonZerosMin\",\"nonZerosMax\"]\n",
        "          for j,k in enumerate(re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)):\n",
        "            variables[variablesValue[j]] = float(k)\n",
        "        else:\n",
        "          variables[\"nonZeros\"] = float(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1])\n",
        "      if(\"RHS nonzeros\" in i):\n",
        "        if(\"Min\" in i):\n",
        "          variablesValue = [\"rhsNonZerosMin\",\"rhsNonZerosMax\"]\n",
        "          for j,k in enumerate(re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)):\n",
        "            variables[variablesValue[j]] = float(k)\n",
        "        else:\n",
        "          variables[\"rhsNonZeros\"] = float(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1])\n",
        "      if(\"CPXPARAM_TimeLimit\" in i):\n",
        "        variables[\"CPXPARAM_TimeLimit\"] = float(i.replace(\"\\n\",\"\").split(\" \")[-1])\n",
        "      if(\"MIP Presolve eliminated\" in i):\n",
        "        variables[\"mipPresolveEliminated\"].append([int(k) for k in re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)])\n",
        "      if(\"MIP Presolve modified \" in i):\n",
        "        variables[\"mipPresolveModified\"].append(int(re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)[0]))\n",
        "      if(\"Reduced MIP has\" in i):\n",
        "        if(\"indicators.\" in i):\n",
        "          tmp = [int(k) for k in re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)]\n",
        "          variables[\"reducedMipHasBinaries\"].append(tmp[0])\n",
        "          variables[\"reducedMipHasGeneral\"].append(tmp[1])\n",
        "        else:\n",
        "          tmp = [int(k) for k in re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)]\n",
        "          variables[\"reducedMipHasColumns\"].append(tmp[1])\n",
        "          variables[\"reducedMipHasNonZero\"].append(tmp[-1])\n",
        "          reduceHasGeneral = []\n",
        "      if(\"Clique\" in i):\n",
        "        variables[\"cliqueTableMembers\"].append(float(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1]))\n",
        "      if(\"Aggregator did\" in i):\n",
        "        variables[\"aggregatorDid\"].append(int(re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)[0]))\n",
        "      if(\"Root relaxation\" in i):\n",
        "        tmp = [float(k) for k in re.findall('-?\\ *[0-9]+\\.?[0-9]*(?:[Ee]\\ *[-+]?\\ *[0-9]+)?',i)]\n",
        "        variables[\"rootRelaxSolSeconds\"].append(tmp[0])\n",
        "        variables[\"rootRelaxSolTicks\"].append(tmp[1])\n",
        "      if(\"Lift and\" in i):\n",
        "        variables[\"liftAndProjectCuts\"] = int(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1])\n",
        "      if(\"Gomory fractional\" in i):\n",
        "        variables[\"gomoryFract\"] = int(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\":\")[1])\n",
        "\n",
        "    df=pd.DataFrame.from_dict(tables).rename(columns={\"seconds\":\"seconds\",\"ticks\":\"ticks\",\"solution\":\"solution\",\"Node\":\"node\",\"Left\":\"nodesLeft\",\"Objective\":\"objective\",\"IInf\":\"iinf\",\"Integer\":\"bestInteger\",\"Bound\":\"BestBound\",\"ItCnt\":\"itCnt\",\"Gap\":\"gap\",\"cuts\":\"cuts\"})\n",
        "    ## Ajuste de lectura de datos del log CPLEX\n",
        "    df = df[ df['bestInteger'] > 10 ]\n",
        "\n",
        "    df['seconds'] = df['seconds'].fillna(0)\n",
        "    eps=np.arange(0, 1, 1/(len(df)), dtype=float)\n",
        "    # df['eps'] = eps\n",
        "    # sum_column = df[\"seconds\"] + df[\"eps\"]\n",
        "    # df[\"seconds\"] = sum_column\n",
        "    # df['ticks'] = df['ticks'].fillna(0)\n",
        "    # eps=np.arange(0, 1, 1/(len(df)), dtype=float)\n",
        "    # df['eps'] = eps\n",
        "    # sum_column = df[\"ticks\"] + df[\"eps\"]\n",
        "    # df[\"ticks\"] = sum_column\n",
        "    return df,variables\n",
        "\n",
        "## Test ExtractCPLEX\n",
        "# e = ExtractCPLEX()\n",
        "# dfe, dic = e.extract(\"/content/logfileMilpuc_022.log\")\n",
        "# print(dfe)\n",
        "# dfe.plot.line( x='seconds',y='bestInteger',style='o-',label='_nolegend_')\n"
      ],
      "metadata": {
        "id": "dA6ZTFMF8Dxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Extract:\n",
        "  def __init__(self,log):\n",
        "    self.log = log \n",
        "  def getFilelog(self,f):\n",
        "    with open(f, 'r') as file:\n",
        "      data = file.read().replace('\\n', '')\n",
        "    tmp = []\n",
        "    for i in re.findall(r\"\\w*\"+self.log+\"\\s*\\'*[a-zA-z.+]*\\'\",data):\n",
        "      tmp.append(i.replace(\"'\",\"\").split(\" \")[1])\n",
        "    return tmp\n",
        "  def extract(self):\n",
        "    fn= \"/content/\"+self.log+\".log\"\n",
        "    variables = {}\n",
        "    variables[\"logFile\"] = self.log.replace(\"Log\",\"uc\")\n",
        "    f = open(fn, \"r\")\n",
        "    for i in f:\n",
        "\n",
        "      variables[\"flag_milp\"] = False\n",
        "      try:\n",
        "        if(\"z_milp=\" in i):\n",
        "          variables[\"z_milp\"    ] = float(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\"=\")[1])\n",
        "          variables[\"flag_milp\" ] = True\n",
        "      except:\n",
        "        print('!!!Error en z_milp')\n",
        "        variables[\"z_milp\"   ] = 1E+75\n",
        "        variables[\"flag_milp\"] = True\n",
        "      try:\n",
        "        if(\"t_milp=\" in i):\n",
        "          variables[\"t_milp\"]  = float(i.replace(\" \",\"\").replace(\"\\n\",\"\").split(\"=\")[1])\n",
        "      except:\n",
        "        print('!!!Error en t_milp')\n",
        "\n",
        "    start = [\"lbc1 results\",\"lbc2 results\",\"lbc3 results\",\"lbc4 results\",\"KS results\"]\n",
        "    stop  = [\"lbc1 end\"    ,\"lbc2 end\"    ,\"lbc3 end\"    ,\"lbc4 end\"    ,\"KS end\"    ]\n",
        "    for p in range(5):\n",
        "      pattern_start = re.compile(r\"\"+start[p])\n",
        "      pattern_stop  = re.compile(r\"\"+\"^\"+stop[p])\n",
        "      i = 0\n",
        "      extract_on = False\n",
        "      extracts = []\n",
        "      with open(r''+fn, 'rt') as myfile:\n",
        "          for line in myfile:\n",
        "              i += 1  # line counting starts with 1\n",
        "              if pattern_start.match(line):\n",
        "                  extract_on = True\n",
        "              if pattern_stop.search(line):\n",
        "                  extract_on = False\n",
        "              if extract_on:\n",
        "                  extracts.append((i, line.rstrip('\\n')))\n",
        "      listlines=[]; x=[]; y=[]\n",
        "      for line in extracts:\n",
        "        listlines.append(line[1])\n",
        "      if len(listlines)!=0:\n",
        "        del listlines[0]\n",
        "      for i in listlines:\n",
        "        if float(re.split(',',i.replace(\" \",\"\"))[1])!=1E+75:\n",
        "          x.append(float(re.split(',',i.replace(\" \",\"\"))[0]))\n",
        "          y.append(float(re.split(',',i.replace(\" \",\"\"))[1]))\n",
        "      if p==0:\n",
        "        variables[\"t_lbc1\"] = x\n",
        "        variables[\"z_lbc1\"] = y\n",
        "        variables[\"flag_lbc1\"] = True\n",
        "      if p==1:\n",
        "        variables[\"t_lbc2\"] = x\n",
        "        variables[\"z_lbc2\"] = y\n",
        "        variables[\"flag_lbc2\"] = True\n",
        "      if p==2:\n",
        "        variables[\"t_lbc3\"] = x\n",
        "        variables[\"z_lbc3\"] = y\n",
        "        variables[\"flag_lbc3\"] = True\n",
        "      if p==3:\n",
        "        variables[\"t_lbc4\"] = x\n",
        "        variables[\"z_lbc4\"] = y\n",
        "        variables[\"flag_lbc4\"] = True\n",
        "      if p==4:\n",
        "        variables[\"t_KS\"] = x\n",
        "        variables[\"z_KS\"] = y\n",
        "        variables[\"flag_KS\"] = True\n",
        "    return variables\n",
        "\n",
        "if False:\n",
        "  ex         = Extract('Log_112')\n",
        "  variablesx = ex.extract()\n",
        "  print(variablesx)"
      ],
      "metadata": {
        "id": "caTKOCID0KvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graficas(title='',graphs=[]):\n",
        "  df1      = df[df['nameins'] == title+'.json' ]\n",
        "  timefull = int(df1.timefull)\n",
        "  plt.figure()\n",
        "  mpl.rcParams['font.size'] = fontsize -2\n",
        "  ## Estandarizamos eje 'y'\n",
        "  ax = plt.gca()\n",
        "  ax.get_yaxis().get_major_formatter().set_useOffset(False)\n",
        "\n",
        "  models = ['LB1','LB2','LB3','LB4','KS','MILP']\n",
        "  i = 0\n",
        "  group_=''\n",
        "  for tup in graphs:\n",
        "      x = tup[0]\n",
        "      y = tup[1]\n",
        "  ##  Recortamos a timefull\n",
        "      x1=[]\n",
        "      y1=[]\n",
        "      index = 0\n",
        "      for j in x:\n",
        "        if j < timefull:\n",
        "          x1.append(j)\n",
        "          y1.append(y[index])\n",
        "        else:\n",
        "          x1.append(timefull)\n",
        "          y1.append(y[index])\n",
        "          break\n",
        "        index = index + 1\n",
        "      if tup[2]:\n",
        "        plt.plot(x1, y1, '.-', label = models[i],markersize=8,)\n",
        "      i=i+1\n",
        "\n",
        "  ##  Extraemos Milp2\n",
        "  # if False:\n",
        "  #   e        = ExtractCPLEX()\n",
        "  #   df3, dic = e.extract(\"/content/logfileMilp2\"+title+\".log\")  \n",
        "  #   x        = df3['seconds'].values.tolist()\n",
        "  #   y        = df3['bestInteger'].values.tolist()\n",
        "  #   xr       = np.array(x)\n",
        "  #   t_harduc = int(df1.t_harduc)\n",
        "  #   xr       = xr + t_harduc           ##  Agregamos los segundos de Harduc\n",
        "  #   groupx   = str(df1.groupx.iloc[0])\n",
        "  #   x1=[]  \n",
        "  #   y1=[]\n",
        "  #   index = 0\n",
        "  #   for j in xr:                       ##  Recortamos MILP2 a timefull\n",
        "  #     if j < timefull:\n",
        "  #       x1.append(j)\n",
        "  #       y1.append(y[index])\n",
        "  #     else:\n",
        "  #       x1.append(timefull)\n",
        "  #       y1.append(y[index])\n",
        "  #       break\n",
        "  #     index = index + 1\n",
        "  # plt.plot(x1,y1,'x',Label = models[6],color='magenta')\n",
        "\n",
        "  ##  Extraemos Milp\n",
        "  try:\n",
        "    e        = ExtractCPLEX()\n",
        "    df4, dic = e.extract(\"/content/logfileMilp\"+title+\".log\")\n",
        "    x        = df4['seconds'    ].values.tolist()\n",
        "    y        = df4['bestInteger'].values.tolist()\n",
        "    plt.plot(x,y,'x',Label = 'MILP')#,color='#ff6666'\n",
        "  except:\n",
        "    print('!!!Error en MILP '+title)\n",
        "\n",
        "  plt.xlabel(\"Time (seconds)\")\n",
        "  plt.ylabel(\"z ($)\")\n",
        "  plt.title('Instance '+title+'\\n ('+filter+')') ###########################################################################\n",
        "  plt.legend(bbox_to_anchor=(1, 0.75))\n",
        "\n",
        "\n",
        "  if title == 'uc_122':\n",
        "    plt.ylim([8.13550E+7,8.139E+7])\n",
        "  if title == 'uc_124':\n",
        "    plt.ylim([8.0347E+7,8.037E+7])\n",
        "\n",
        "  if title == 'uc_062':\n",
        "    plt.ylim([3.2365E+7,3.24E+7])\n",
        "  if title == 'uc_069':\n",
        "    plt.ylim([3.287E+7,3.291E+7])\n",
        "  if title == 'uc_101':\n",
        "    plt.ylim([3.84E+7,3.848E+7])\n",
        "  if title == 'uc_103':\n",
        "    plt.ylim([4.234E+7,4.236E+7])\n",
        "\n",
        "  if title == 'uc_075':\n",
        "    plt.ylim([1.1214E+8,1.1217E+8])\n",
        "  if title == 'uc_077':\n",
        "    plt.ylim([1.27066E+8,1.271E+8])\n",
        "  if title == 'uc_147':\n",
        "    plt.ylim([1.5104E+8,1.5108E+8])\n",
        "  if title == 'uc_149':\n",
        "    plt.ylim([1.566155E+8,1.56641E+8])\n",
        "  if title == 'uc_151':\n",
        "    plt.ylim([1.5574E+8,1.55775E+8])\n",
        "\n",
        "  if title == 'uc_083':\n",
        "    plt.ylim([4.5572E+7,4.5605E+7])\n",
        "  if title == 'uc_084':\n",
        "    plt.ylim([4.5245E+7,4.5295E+7])\n",
        "  if title == 'uc_086':\n",
        "    plt.ylim([3.437E+7,3.442E+7])\n",
        "  if title == 'uc_088':\n",
        "    plt.ylim([4.6023E+7,4.6055E+7])\n",
        "  if title == 'uc_089':\n",
        "    plt.ylim([5.5935E+7,5.596E+7])\n",
        "  if title == 'uc_090':\n",
        "    plt.ylim([5.05055E+7,5.0510E+7])\n",
        "\n",
        "  # timefull = int(df1.timefull)\n",
        "  # timeconst= int(df1.timeconst)\n",
        "  # plt.xlim([timeconst, timefull])\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(dir+'fig:'+title+'_'+filter+'.pdf')\n",
        "  # plt.show()\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "0CaDNDtj5Ktu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gráficos de convergencia "
      ],
      "metadata": {
        "id": "X0QTkXRFPT95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res=[] ## Vector de resultados de todos los métodos\n",
        "for item in list:\n",
        "    graphs = []\n",
        "    try:\n",
        "      e = Extract(item)\n",
        "      variables = e.extract()\n",
        "      if True:\n",
        "        #variables[\"flag_harjk\"] = False\n",
        "        # variables[\"flag_KS\"   ] = False\n",
        "        # variables[\"flag_lbc1\" ] = False\n",
        "        # variables[\"flag_lbc2\" ] = False\n",
        "        # variables[\"flag_lbc3\" ] = False\n",
        "        # variables[\"flag_lbc4\" ] = False\n",
        "        # graphs.append(([variables['t_harjk']],[variables['z_harjk']],variables[\"flag_harjk\"]))\n",
        "        # graphs.append(([variables['t_milp2']],[variables['z_milp2']],variables[\"flag_milp2\"]))       \n",
        "        # graphs.append(([variables['t_hard3']],[variables['z_hard3']],variables[\"flag_hard3\"])) \n",
        "        # graphs.append(([variables['t_milp' ]],[variables['z_milp' ]],variables[\"flag_milp\" ]))\n",
        "        graphs.append(( variables['t_lbc1' ] , variables['z_lbc1'  ] , variables[\"flag_lbc1\" ]))\n",
        "        graphs.append(( variables['t_lbc2' ] , variables['z_lbc2'  ] , variables[\"flag_lbc2\" ]))\n",
        "        graphs.append(( variables['t_lbc3' ] , variables['z_lbc3'  ] , variables[\"flag_lbc3\" ]))\n",
        "        graphs.append(( variables['t_lbc4' ] , variables['z_lbc4'  ] , variables[\"flag_lbc4\" ]))\n",
        "        graphs.append(( variables['t_KS'   ] , variables['z_KS'    ] , variables[\"flag_KS\"   ]))\n",
        "        ## Función que grafica e incluye MILP\n",
        "        a,b = graficas(title=variables[\"logFile\"],graphs=graphs)\n",
        "        graphs.append((a,b,True))\n",
        "        res.append(graphs)\n",
        "    except Exception as err:\n",
        "      print(\"Fail in \", item+\".log\"+ \" file\")\n",
        "      print(str(err))"
      ],
      "metadata": {
        "id": "st05UPVm3yQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtenemos informaciòn de MILP2"
      ],
      "metadata": {
        "id": "aEapAG3_NppS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extrae_MILP2(title=''):\n",
        "  df1      = df[df['nameins'] == title+'.json' ]\n",
        "  print(title+'.json')\n",
        "  timefull = int(df1.timefull)\n",
        "\n",
        "  #  Extraemos Milp2\n",
        "  if True:\n",
        "    e        = ExtractCPLEX()\n",
        "    df3, dic = e.extract(\"/content/logfileMilp2\"+title+\".log\")  \n",
        "    x        = df3['seconds'].values.tolist()\n",
        "    y        = df3['bestInteger'].values.tolist()\n",
        "    xr       = np.array(x)\n",
        "    t_harduc = int(df1.t_harduc)\n",
        "    xr       = xr + t_harduc           ##  Agregamos los segundos de Harduc\n",
        "    groupx   = str(df1.groupx.iloc[0])\n",
        "    x1=[]  \n",
        "    y1=[]\n",
        "    index = 0\n",
        "    \n",
        "  ##  Extraemos Milp\n",
        "  # try:\n",
        "  #   e        = ExtractCPLEX()\n",
        "  #   df4, dic = e.extract(\"/content/logfileMilp\"+title+\".log\")\n",
        "  #   x        = df4['seconds'    ].values.tolist()\n",
        "  #   y        = df4['bestInteger'].values.tolist()\n",
        "  #   plt.plot(x,y,'x',Label = 'MILP',color='#ff6666')\n",
        "  # except:\n",
        "  #   print('!!!Error en MILP '+title)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "Q7mpX44LE7iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res2=[] ## Vector de resultados de todos los métodos Y MILP2\n",
        "for item in list:\n",
        "    graphs2 = []\n",
        "    #try:\n",
        "    if True:\n",
        "      e = Extract(item)\n",
        "      variables = e.extract()\n",
        "      if True:\n",
        "        ## Función que extrae MILP2\n",
        "        c,d = extrae_MILP2(title=variables[\"logFile\"])\n",
        "        graphs2.append((c,d,True))\n",
        "        res2.append(graphs2)\n",
        "    # except Exception as err:\n",
        "    #   print(\"Fail in \", item+\".log\"+ \" file\")\n",
        "    #   print(str(err))"
      ],
      "metadata": {
        "id": "K0ddn72ZD4e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estudio a una hora"
      ],
      "metadata": {
        "id": "jJBANQZSDalQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lbc1_1h=[]; lbc2_1h=[]; lbc3_1h=[]; lbc4_1h=[]; ks_1h=[]; milp_1h=[]; milp2_1h=[]\n",
        "seconds = 4000\n",
        "k       = 0 \n",
        "valor   = 0\n",
        "for item in res:  \n",
        "  i=0     ## lbc1_1h\n",
        "  for x in item[0][0]:\n",
        "    if x>seconds:  \n",
        "      break\n",
        "    valor = item[0][1][i]\n",
        "    i=i+1\n",
        "  lbc1_1h.append(gap(lb_milp[k],valor))\n",
        " \n",
        "  valor=0  ## lbc2_1h\n",
        "  i=0   \n",
        "  for x in item[1][0]:\n",
        "    if x>seconds: \n",
        "      break\n",
        "    valor = item[1][1][i]\n",
        "    i=i+1\n",
        "  lbc2_1h.append(gap(lb_milp[k],valor))\n",
        "\n",
        "  valor=0  ## lbc3_1h\n",
        "  i=0   \n",
        "  for x in item[2][0]:\n",
        "    if x>seconds: \n",
        "      break\n",
        "    valor = item[2][1][i]\n",
        "    i=i+1\n",
        "  lbc3_1h.append(gap(lb_milp[k],valor))\n",
        "\n",
        " \n",
        "  valor=0  ## lbc4_1h\n",
        "  i=0   \n",
        "  for x in item[3][0]:\n",
        "    if x>seconds: \n",
        "      break\n",
        "    valor = item[3][1][i]\n",
        "    i=i+1\n",
        "  lbc4_1h.append(gap(lb_milp[k],valor))\n",
        "\n",
        " \n",
        "  valor=0  ## ks_1h  \n",
        "  i=0   \n",
        "  for x in item[4][0]:\n",
        "    if x>seconds: \n",
        "      break\n",
        "    valor = item[4][1][i]\n",
        "    i=i+1\n",
        "  ks_1h.append(gap(lb_milp[k],valor))\n",
        "\n",
        "\n",
        "  valor=0  ## milp_1h\n",
        "  i=0   \n",
        "  for x in item[5][0]:\n",
        "    if x>seconds: \n",
        "      break\n",
        "    valor = item[5][1][i]\n",
        "    i=i+1\n",
        "  milp_1h.append(gap(lb_milp[k],valor))  \n",
        "\n",
        "  \n",
        "  valor=0  ## milp2_1h\n",
        "  i=0   \n",
        "  for x in item[0][0]:\n",
        "    if x>seconds: \n",
        "      break\n",
        "    valor = item[0][1][i]\n",
        "    i=i+1\n",
        "  milp2_1h.append(gap(lb_milp[k],valor))\n",
        "\n",
        "  k=k+1"
      ],
      "metadata": {
        "id": "R0wnRPRTDav7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estadísticos descriptivos a UNA HORA\n"
      ],
      "metadata": {
        "id": "w4FP31IkKFlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Estadísticos descriptivos a UNA HORA\n",
        "print('Label, tabla:descriptive_'+filter)\n",
        "print('Caption,','Descriptive statistics of method results ('+ filter+')')\n",
        "print('gap mean,gap std,n')\n",
        "print('LB1_1h',np.average(lbc1_1h),',',np.std(lbc1_1h),','  ,)\n",
        "print('LB2_1h',np.average(lbc2_1h),',',np.std(lbc2_1h),','  , )\n",
        "print('LB3_1h',np.average(lbc3_1h),',',np.std(lbc3_1h),','  ,)\n",
        "print('LB4_1h',np.average(lbc4_1h),',',np.std(lbc4_1h),','  , )\n",
        "print('KS_1h',np.average(ks_1h),',',np.std(ks_1h),','      , )\n",
        "print('MILP_1h',np.average(milp_1h),',',np.std(milp_1h),','  , )\n",
        "print('MILP2_1h',np.average(milp2_1h),',',np.std(milp2_1h),',',)\n"
      ],
      "metadata": {
        "id": "ixnyvKQgBwoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## gap a una hora \n",
        "lbc1_1h  = np.asarray(lbc1_1h)\n",
        "lbc2_1h  = np.asarray(lbc2_1h)\n",
        "lbc3_1h  = np.asarray(lbc3_1h)\n",
        "lbc4_1h  = np.asarray(lbc4_1h)\n",
        "ks_1h    = np.asarray(ks_1h)\n",
        "milp_1h  = np.asarray(milp_1h)\n",
        "milp2_1h  = np.asarray(milp2_1h)\n",
        "print(len(lbc1_1h),len(lbc2_1h),len(lbc3_1h),len(lbc4_1h),len(ks_1h),len(milp_1h),len(milp2_1h))"
      ],
      "metadata": {
        "id": "9J5rixB8DL3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##gap de los que si se obtuvo un LB del MILP\n",
        "lbc1_1h  = lbc1_1h[~np.isnan(lbc1_1h)] ## limpiamos NaN\n",
        "lbc2_1h  = lbc2_1h[~np.isnan(lbc2_1h)]\n",
        "lbc3_1h  = lbc3_1h[~np.isnan(lbc3_1h)]\n",
        "lbc4_1h  = lbc4_1h[~np.isnan(lbc4_1h)]\n",
        "ks_1h    = ks_1h  [~np.isnan(ks_1h  )]\n",
        "milp_1h  = milp_1h[~np.isnan(milp_1h)]   \n",
        "milp2_1h  = milp2_1h[~np.isnan(milp2_1h)]   \n",
        "print(len(lbc1_1h),len(lbc2_1h),len(lbc3_1h),len(lbc4_1h),len(ks_1h),len(milp_1h),len(milp2_1h))             "
      ],
      "metadata": {
        "id": "PcrCBDJeNSFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels      = ['LB1_1h','LB2_1h','LB3_1h','LB4_1h','KS','MILP1h\\n('+str(len(milp_1h))+'/'+str(n)+')','MILP2_1h']\n",
        "x           = [lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h]\n",
        "medianprops = dict(color='black') #https://matplotlib.org/stable/gallery/statistics/boxplot.html\n",
        "fig,ax  = plt.subplots()\n",
        "#ax.set_title('gap\\n('+filter+')')\n",
        "bp = ax.boxplot(x,           \n",
        "positions=[1,2,3,4,5,6,7],              # where to put these arrays\n",
        "labels = labels, patch_artist=True ,medianprops=medianprops)  # allow filling the box with colors\n",
        "## Relative gap in one hour\n",
        "ax.set_title(filter+'\\n 1 hour', fontsize=fontsize)\n",
        "ax.set_xlabel('Methods'  , fontsize=fontsize)\n",
        "ax.set_ylabel('gap'      , fontsize=fontsize)\n",
        "for i in range(len(x)):\n",
        "  bp['boxes'][i].set_facecolor('black')\n",
        "  bp['boxes'][i].set_color(str(palette[i]))\n",
        "\n",
        "fig.tight_layout()\n",
        "namefile = dir+'fig:boxplotgap_1h'+'_'+filter+'.pdf'\n",
        "plt.savefig(namefile, transparent=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sGH-KXr9OKl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## https://www.geeksforgeeks.org/data-visualization-with-python-seaborn/\n",
        "method1 = ['LB1_1h'   ]*(len(lbc1_1h))\n",
        "method2 = ['LB2_1h'   ]*(len(lbc2_1h))\n",
        "method3 = ['LB3_1h'   ]*(len(lbc3_1h))\n",
        "method4 = ['LB4_1h'   ]*(len(lbc4_1h))\n",
        "method5 = ['KS_1h'    ]*(len(ks_1h))\n",
        "method6 = ['MILP1h\\n('+str(len(milp_1h))+'/'+str(n)+')' ]*(len(milp_1h))\n",
        "method7 = ['MILP2_1h']*(len(milp2_1h))\n",
        "method  = method1 + method2 + method3 + method4 + method5 + method6 + method7\n",
        "data    = np.concatenate((lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h), axis=0)\n",
        "# initialise data of lists.\n",
        "data = {'gap'       :data,\n",
        "        'method'  :method}  \n",
        "df1 = pd.DataFrame( data )\n",
        "#fig, ax = plt.subplots(figsize=(8, 6))\n",
        "#sns.histplot(df1, x=\"gap\", hue=\"method\", element=\"step\") #,bins=15"
      ],
      "metadata": {
        "id": "QXstzuDWvoek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "  import seaborn as sns\n",
        "  fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(9, 8))\n",
        "  sns.violinplot(\n",
        "      x       = df1.gap,\n",
        "      y       = df1.method,\n",
        "      color   = '.8',\n",
        "      ax      = axs[0]  )\n",
        "  sns.stripplot(\n",
        "      palette=palette,\n",
        "      x       = df1.gap,\n",
        "      y       = df1.method,\n",
        "      data    = df1,\n",
        "      size    = 4,\n",
        "      jitter  = 0.1,\n",
        "      ax      = axs[0]  )\n",
        "  #axs[0].set_title('Distribution of methods')\n",
        "  axs[0].set_ylabel('Methods')\n",
        "  axs[0].set_title(filter+'\\n 1 hour', fontsize=fontsize)\n",
        "  axs[0].set_xlabel('GAP'            , fontsize=fontsize)\n",
        "  axs[0].set_ylabel('Methods'        , fontsize=fontsize)\n",
        "\n",
        "  for i in df1.method.unique():\n",
        "      datos_temp = df1[df1.method == i]['gap']\n",
        "      datos_temp.plot.kde(ax=axs[1], label=i)\n",
        "      axs[1].plot(datos_temp, np.full_like(datos_temp, 0), '|k', markeredgewidth=1)\n",
        "\n",
        "  axs[1].legend()\n",
        "  axs[1].set_xlabel('GAP'       , fontsize=fontsize)\n",
        "  axs[1].set_ylabel('Frecuency' , fontsize=fontsize)\n",
        "  fig.tight_layout()\n",
        "  namefile = dir+'fig:violinAll_1h'+'_'+filter+'.pdf'\n",
        "  plt.savefig(namefile, transparent=True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "eaO8jOLvwRXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de resultados de stat\n"
      ],
      "metadata": {
        "id": "-O6z-YGJm-HN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Imprimir GAP\n",
        "#milp2   = pd.to_numeric(df.g_milp2 ).to_numpy()\n",
        "harjk   = pd.to_numeric(df.g_harjk ).to_numpy()\n",
        "harduc  = pd.to_numeric(df.g_harduc).to_numpy()\n",
        "lbc1    = pd.to_numeric(df.g_lbc1  ).to_numpy()\n",
        "lbc2    = pd.to_numeric(df.g_lbc2  ).to_numpy()\n",
        "lbc3    = pd.to_numeric(df.g_lbc3  ).to_numpy()\n",
        "lbc4    = pd.to_numeric(df.g_lbc4  ).to_numpy()\n",
        "ks      = pd.to_numeric(df.g_ks    ).to_numpy()\n",
        "milp    = pd.to_numeric(df.g_milp  ).to_numpy()\n",
        "milp2    = pd.to_numeric(df.g_milp2  ).to_numpy()\n",
        "\n",
        "## Contamos las soluciones encontradas y no encontradas\n",
        "n_milp_ns   = len(milp  [np.isnan(milp)  ])\n",
        "n_milp2_ns   = len(milp2  [np.isnan(milp2)  ])\n",
        "n_harjk_ns  = len(harjk [np.isnan(harjk) ])\n",
        "n_harduc_ns = len(harduc[np.isnan(harduc)])\n",
        "n_milp      = len(milp  [~np.isnan(milp)  ])\n",
        "n_milp2      = len(milp2  [~np.isnan(milp2)  ])\n",
        "n_harjk     = len(harjk [~np.isnan(harjk) ])\n",
        "n_harduc    = len(harduc[~np.isnan(harduc)])\n",
        "\n",
        "print(n,n_milp_ns,n_milp2_ns,n_harjk_ns,n_harduc_ns)\n",
        "\n",
        "df = df[df['g_milp'].notna()]\n",
        "harjk   = pd.to_numeric(df.g_harjk ).to_numpy()\n",
        "harduc  = pd.to_numeric(df.g_harduc).to_numpy()\n",
        "lbc1    = pd.to_numeric(df.g_lbc1  ).to_numpy()\n",
        "lbc2    = pd.to_numeric(df.g_lbc2  ).to_numpy()\n",
        "lbc3    = pd.to_numeric(df.g_lbc3  ).to_numpy()\n",
        "lbc4    = pd.to_numeric(df.g_lbc4  ).to_numpy()\n",
        "ks      = pd.to_numeric(df.g_ks    ).to_numpy()\n",
        "milp    = pd.to_numeric(df.g_milp  ).to_numpy()\n",
        "milp2   = pd.to_numeric(df.g_milp2  ).to_numpy()\n",
        "#milp    = milp[~np.isnan(milp)]                 ## limpiamos NaN\n",
        "#harjk  = harjk[~np.isnan(harjk)]                ## limpiamos NaN\n",
        "\n",
        "labels      = ['LB1','LB2','LB3','LB4','KS','MILP\\n('+str(len(milp))+'/'+str(n)+')','MILP2']\n",
        "medianprops = dict(color='black') #https://matplotlib.org/stable/gallery/statistics/boxplot.html\n",
        "x           = [lbc1,lbc2,lbc3,lbc4,ks,milp,milp2]\n",
        "fig,ax      = plt.subplots()\n",
        "#ax.set_title('gap\\n('+filter+')')\n",
        "bp = ax.boxplot(x,           \n",
        "positions=[1,2,3,4,5,6,7],              # where to put these arrays\n",
        "labels = labels, patch_artist=True, medianprops=medianprops )  # allow filling the box with colors\n",
        "## Relative gap in two hour\n",
        "ax.set_title(filter+'\\n 2 hours', fontsize=fontsize)\n",
        "ax.set_xlabel('Methods'  , fontsize=fontsize)\n",
        "ax.set_ylabel('GAP'      , fontsize=fontsize)\n",
        "for i in range(len(x)):\n",
        "  bp['boxes'][i].set_facecolor('black')\n",
        "  bp['boxes'][i].set_color(str(palette[i]))\n",
        "fig.tight_layout()\n",
        "namefile = dir+'fig:boxplotgap'+'_'+filter+'.pdf'\n",
        "plt.savefig(namefile, transparent=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IqsCfW-H-4pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis estadístico descriptivo"
      ],
      "metadata": {
        "id": "lEqH2fdnGTz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 'Solutions obtained by the Solver'\n",
        "pie(title='MILP\\n'+filter,labels=['Feasible solutions\\n found','Feasible solutions\\n not found', ],sizes=[n-n_milp_ns, n_milp_ns ],colors=['#ff6666','#66b3ff','#99ff99'],label='MILP')\n",
        "## 'Solutions obtained by the Harjunkosky constructive'\n",
        "#pie(title='Harjunkosky\\n'+filter,labels=['Solution\\n found','Solutions\\n not found', ],sizes=[n-n_harjk_ns,n_harjk_ns],label='Harjunkosky')\n"
      ],
      "metadata": {
        "id": "Ieh3AcyPPhpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estudiando la solución inicial del constructivo"
      ],
      "metadata": {
        "id": "8abB78G11v9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "milpc=[]\n",
        "seconds = 1200\n",
        "if filter=='x10gen_small':\n",
        "  seconds = 350\n",
        "elif filter=='x10gen_large':\n",
        "  seconds = 900 ## ó 350,1200\n",
        "k       = 0 \n",
        "valor   = 0\n",
        "for item in res:  \n",
        "  valor=0  ## milp_1h\n",
        "  i=0   \n",
        "  for x in item[5][0]:\n",
        "    if x>seconds: \n",
        "      break\n",
        "    valor = item[5][1][i]\n",
        "    i=i+1\n",
        "  milpc.append(gap(lb_milp[k],valor))\n",
        "  k=k+1\n",
        "milpc  = np.asarray(milpc)\n",
        "milpc  = milpc[~np.isnan(milpc)]    ## limpiamos NaN\n",
        "n_milpc =len(milpc)\n",
        "n_milpc_ns = n - n_milpc\n",
        "milpc"
      ],
      "metadata": {
        "id": "YukkvMM71fYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 'Solutions obtained by the Harjunkosky constructive'\n",
        "#pie(title='Harjunkosky\\n'+filter,labels=['Feasible\\n solution\\n found','Feasible\\n solutions\\n not found'],sizes=[n-n_harjk_ns,n_harjk_ns  ],label='Harjunkosky')\n",
        "#pie(title='Harduc\\n'     +filter,labels=['Feasible\\n solution\\n found','Feasible\\n solutions\\n not found'],sizes=[n-n_harduc_ns,n_harduc_ns],label='Harduc')\n",
        "#pie(title='MILPc\\n'      +filter,labels=['Feasible\\n solution\\n found','Feasible\\n solutions\\n not found'],sizes=[n-n_MILPc_ns,n_MILPc_ns  ],label='MILPc')\n"
      ],
      "metadata": {
        "id": "gaOjeuYtfmaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "labels = ['Harduc\\n('+str(n_harduc)+'/'+str(n)+')','Harjk\\n('+str(n_harjk)+'/'+str(n)+')','MILPc\\n('+str(n_milpc)+'/'+str(n)+')']\n",
        "counts = [n_harduc, n_harjk, n_milpc]\n",
        "bar_labels = ['red', 'blue', '_red']\n",
        "per_counts=[trunc((n_harduc/n)*100,1), trunc((n_harjk/n)*100,1), trunc((n_milpc/n)*100,1)]\n",
        "\n",
        "ax.bar(labels, counts, color=palette)\n",
        "ax.set_ylabel('Feasible solutions')\n",
        "ax.set_title(filter,fontsize=fontsize)\n",
        "\n",
        "for index,data in enumerate(counts):\n",
        "    plt.text(x=index-0.15 , y=data+0.15 , s=f\"{per_counts[index]}\"+\"%\" , fontdict=dict(fontsize=fontsize))\n",
        "plt.tight_layout()\n",
        "\n",
        "namefile = dir+'fig:bars_cons'+'_'+filter+'.pdf'\n",
        "plt.savefig(namefile, transparent=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZLWUC556qHPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## https://www.geeksforgeeks.org/data-visualization-with-python-seaborn/\n",
        "\n",
        "method  = ['Harduc\\n('+str(n_harduc)+'/'+str(n)+')']*(len(harduc))\n",
        "method2 = ['Harjk\\n('+str(n_harjk)  +'/'+str(n)+')']*(len(harjk ))\n",
        "method3 = ['MILPc\\n('+str(n_milpc)  +'/'+str(n)+')']*(len(milpc))\n",
        "method  = method + method2 + method3\n",
        "data    = np.concatenate((harduc,harjk, milpc), axis=0)\n",
        "# initialise data of lists.\n",
        "data = {'gap' :data, 'method' :method}  \n",
        "df1 = pd.DataFrame( data )"
      ],
      "metadata": {
        "id": "Vqcnzos4f0kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estadísticos descriptivos constructivos"
      ],
      "metadata": {
        "id": "CF-DUihYI_on"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Estadísticos descriptivos a UNA HORA\n",
        "print('Label, tab:descriptive_constructive')\n",
        "print('Caption,','Descriptive statistics of constructive methods results')\n",
        "print('gap mean,gap std,n')\n",
        "harjk    = harjk[~np.isnan(harjk)]                 ## limpiamos NaN\n",
        "harduc    = harduc[~np.isnan(harduc)]                 ## limpiamos NaN\n",
        "milpc    = milpc[~np.isnan(milpc)]                 ## limpiamos NaN\n",
        "print('MILPc,', filter,',',np.average(milpc),',',np.std(milpc),',',n_milpc)\n",
        "print('HARJK,', filter,',',np.average(harjk),',',np.std(harjk),',',n_harjk)\n",
        "print('Harduc,', filter,',',np.average(harduc),',',np.std(harduc),',',n_harduc)\n"
      ],
      "metadata": {
        "id": "RKEM_9HPHet0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=0;b=0.002\n",
        "if filter == 'x7day_large':\n",
        "  a=0.0;b=0.002\n",
        "if filter == 'x7day_small':\n",
        "  a=0.0;b=0.004\n",
        "\n",
        "if True:\n",
        "  import seaborn as sns\n",
        "  fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(9, 8))\n",
        "  sns.violinplot(\n",
        "      x       = df1.gap,\n",
        "      y       = df1.method,\n",
        "      color   = '.8',\n",
        "      ax      = axs[0]  )\n",
        "  sns.stripplot(\n",
        "      x       = df1.gap,\n",
        "      y       = df1.method,\n",
        "      data    = df1,\n",
        "      size    = 4,\n",
        "      jitter  = 0.1,\n",
        "      ax      = axs[0] )\n",
        "  #axs[0].set_title('Distribución de gap por método')\n",
        "  axs[0].set_ylabel('methods')\n",
        "  ##Distribution of methods\n",
        "  axs[0].set_title(''+filter+'', fontsize=fontsize)\n",
        "  axs[0].set_xlabel('GAP'      , fontsize=fontsize)\n",
        "  axs[0].set_ylabel('Constructive methods'  , fontsize=fontsize)\n",
        "  if filter == 'x7day_large' or filter == 'x7day_small':\n",
        "    axs[0].set_xlim([a,  b])\n",
        "\n",
        "  fig.tight_layout()\n",
        "\n",
        "  for i in df1.method.unique():\n",
        "      datos_temp = df1[df1.method == i]['gap']\n",
        "      datos_temp.plot.kde(ax=axs[1], label=i)\n",
        "      axs[1].plot(datos_temp, np.full_like(datos_temp, 0), '|k', markeredgewidth=1)\n",
        "\n",
        "  # axs[1].set_xlabel('gap')\n",
        "  axs[1].legend()\n",
        "  axs[1].set_xlabel('GAP'       , fontsize=fontsize)\n",
        "  axs[1].set_ylabel('Frecuency' , fontsize=fontsize)\n",
        "\n",
        "  if filter == 'x7day_large' or filter == 'x7day_small':\n",
        "    axs[1].set_xlim([a,  b])\n",
        "\n",
        "  fig.tight_layout()\n",
        "  namefile = dir+'fig:violinHarduc'+'_'+filter+'.pdf'\n",
        "  plt.savefig(namefile, transparent=True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "B1FJEn6LFLQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## https://www.geeksforgeeks.org/data-visualization-with-python-seaborn/\n",
        "method1 = ['LB1'   ]*(len(lbc1))\n",
        "method2 = ['LB2'   ]*(len(lbc2))\n",
        "method3 = ['LB3'   ]*(len(lbc3))\n",
        "method4 = ['LB4'   ]*(len(lbc4))\n",
        "method5 = ['KS'    ]*(len(ks))\n",
        "# method6 = ['MILP'  ]*(len(milp))\n",
        "# method7 = ['MILP2' ]*(len(milp2))\n",
        "method6 = ['MILP\\n('+str(len(milp))+'/'+str(n)+')' ]*(len(milp))\n",
        "method7 = ['MILP2']*(len(milp2))\n",
        "method  = method1 + method2 + method3 + method4 + method5 + method6 + method7\n",
        "data    = np.concatenate((lbc1,lbc2,lbc3,lbc4,ks,milp,milp2), axis=0)\n",
        "# initialise data of lists.\n",
        "data = {'gap'       :data,\n",
        "        'method'  :method}  \n",
        "df1 = pd.DataFrame( data )\n",
        "#fig, ax = plt.subplots(figsize=(8, 6))\n",
        "#sns.histplot(df1, x=\"gap\", hue=\"method\", element=\"step\") #,bins=15"
      ],
      "metadata": {
        "id": "zCh6HJY9arPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "  import seaborn as sns\n",
        "  fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(9, 8))\n",
        "  sns.violinplot(\n",
        "      x       = df1.gap,\n",
        "      y       = df1.method,\n",
        "      color   = '.8',\n",
        "      ax      = axs[0]  )\n",
        "  sns.stripplot(\n",
        "      x       = df1.gap,\n",
        "      y       = df1.method,\n",
        "      data    = df1,\n",
        "      size    = 4,\n",
        "      jitter  = 0.1,\n",
        "      ax      = axs[0]  )\n",
        "  axs[0].set_ylabel('methods')\n",
        "  ## Distribution of methods\n",
        "  axs[0].set_title(filter+'\\n 2 hours', fontsize=fontsize)\n",
        "  axs[0].set_xlabel('GAP'                                 , fontsize=fontsize)\n",
        "  axs[0].set_ylabel('Methods'                             , fontsize=fontsize)\n",
        "  fig.tight_layout()\n",
        "\n",
        "  for i in df1.method.unique():\n",
        "      datos_temp = df1[df1.method == i]['gap']\n",
        "      datos_temp.plot.kde(ax=axs[1], label=i)\n",
        "      axs[1].plot(datos_temp, np.full_like(datos_temp, 0), '|k', markeredgewidth=1)\n",
        "\n",
        "  axs[1].legend()\n",
        "  axs[1].set_xlabel('GAP'       , fontsize=fontsize)\n",
        "  axs[1].set_ylabel('Frecuency' , fontsize=fontsize)\n",
        "  fig.tight_layout()\n",
        "\n",
        "  namefile = dir+'fig:violinAll'+'_'+filter+'.pdf'\n",
        "  plt.savefig(namefile, transparent=True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Ni9Y4gHH8h_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Almacenamos a archivos zip"
      ],
      "metadata": {
        "id": "uLlF_tcrGYqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r file.zip '/content/Figures'"
      ],
      "metadata": {
        "id": "oeRTUFN8GYNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "OiPbEwKavvXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def means_test(alternative,labels,samples,alpha=0.05):\n",
        "## Ho(Accepted): The difference between the pairs follows a symmetric distribution around zero.\n",
        "## Ha(Rejected): The difference between the pairs does not follow a symmetric distribution around zero.\n",
        "## We conduct the Mann-whitney U or two-sample t test alternative= {'two-sided', 'greater', 'less'} \n",
        "# ==============================================================================\n",
        "  ## Tipo de prueba\n",
        "\n",
        "    Ho   = labels[0]+'-'+labels[1]+': means the samples is of the same distribution.'\n",
        "    decision1   = 'We fail to reject the Ho; Ho accepted:'\n",
        "    if alternative == 'less':\n",
        "      message1='\\t: We fail to reject the null hypothesis; Ho accepted: \\n\\tThe difference between the pairs follows a symmetric distribution around zero.'\n",
        "      message2='\\t: *We reject the null hypothesis and accept alternative hypothesis Ha:\\n\\t'+labels[0]+'\\'s mean is less than '+labels[1]+'\\'s mean.'\n",
        "      Ho   = labels[0]+'-'+labels[1]+': The means difference of the samples from the same distribution.'\n",
        "      decision2   = 'We reject the Ho and accept Ha: '+labels[0]+'\\'s mean is less than '+labels[1]+'\\'s mean.'\n",
        "    if alternative == 'greater':\n",
        "      message2='\\t: *We reject the null hypothesis and accept alternative hypothesis Ha:\\n\\t'+labels[0]+'\\'s mean is greater than '+labels[1]+'\\'s mean.'\n",
        "      Ho   = labels[0]+'-'+labels[1]+': The means difference of the samples from the same distribution.'\n",
        "      decision2   = 'We reject the Ho and accept Ha: '+labels[0]+'\\'s mean is greater than '+labels[1]+'\\'s mean.'\n",
        "    if alternative == 'two-sided':\n",
        "      message2='\\t: *We reject the null hypothesis and accept alternative hypothesis Ha:\\n\\t The mean difference between the pairs not follows a symmetric distribution around zero.'\n",
        "      Ho   = labels[0]+'-'+labels[1]+': The means difference of the samples from the same distribution.'\n",
        "      decision2   = 'We reject the Ho and accept Ha: The means difference of the samples not from the same distribution.'\n",
        "\n",
        "    ## Checamos normalidad \n",
        "    normal=[]\n",
        "    i = 0\n",
        "    for s in samples:\n",
        "      if shapiro(s).pvalue > alpha:\n",
        "        normal.append(True)\n",
        "      else:\n",
        "        # print(labels[i],'\\t:',round(shapiro(s).statistic,4),'\\t',round(shapiro(s).pvalue,4),'\\tWe reject the null hypothesis and accept Ha: Sample is not from the normal distributions.')\n",
        "        normal.append(False)\n",
        "      i = i + 1\n",
        "\n",
        "    ## Checamos homocedasticidad\n",
        "    levene_test = stats.levene(samples[0], samples[1], center='mean')\n",
        "    equalvar=False\n",
        "    if levene_test.pvalue > alpha:\n",
        "        equalvar =True\n",
        "    #   print('[Harjk,Harduc]\\t:',round(levene_test.statistic,4),round(levene_test.pvalue,4),'\\tWe fail to reject the null hypothesis; Ho accepted: the variances are equal across all samples. (Po>0.05)')\n",
        "    # else:\n",
        "    #   print(labels[0],labels[1],'\\t:',round(levene_test.statistic,4),round(levene_test.pvalue,4),'\\tWe reject the Ho and accept Ha: the variances are not equal across all samples. (Po<=0.05)')\n",
        "    test = ''\n",
        "    if normal[0]==True and normal[1]==True and equalvar==True: ## T-test two samples\n",
        "      test = 'T-test for two samples'\n",
        "      # print(labels[0],labels[1],stats.ttest_ind(samples[0], samples[1], alternative=alternative) )\n",
        "      if stats.ttest_ind(samples[0], samples[1], alternative=alternative).pvalue > alpha:\n",
        "          # print('\\t',message1)\n",
        "          print(Ho,',',test,',', round(stats.ttest_ind(samples[0], samples[1], alternative=alternative).pvalue,4),',',decision1)\n",
        "      else:\n",
        "          # print('\\t',message2)\n",
        "          print(Ho,',',test,',', round(stats.ttest_ind(samples[0], samples[1], alternative=alternative).pvalue,4),',',decision2)\n",
        "    else: ## Mann-whitney U Test\n",
        "      test = 'Mann-whitney'\n",
        "      # print(labels[0],labels[1],stats.mannwhitneyu(samples[0], samples[1], alternative=alternative) )\n",
        "      if stats.mannwhitneyu(samples[0], samples[1], alternative=alternative).pvalue > alpha:\n",
        "          # print('\\t',message1)\n",
        "          print(Ho,',',test,',', round(stats.mannwhitneyu(samples[0], samples[1], alternative=alternative).pvalue,4),',',decision1)\n",
        "      else:\n",
        "          # print('\\t',message2)\n",
        "          print(Ho,',',test,',', round(stats.mannwhitneyu(samples[0], samples[1], alternative=alternative).pvalue,4),',',decision2)\n",
        "## Kruskal-Wallis test discussion:  In this example, the test statistic comes out to be equal to 87 and the corresponding p-value is 2.1856E-17. \n",
        "## (As the p-value is not less than 0.05, we cannot reject the null hypothesis that the median of optimality gap is the same for all groups. \n",
        "## Hence, We don’t have sufficient proof to claim that the different types of methods used to lead to statistically significant differences in the acuracy of methods.)\n",
        "## (As the p-value is less than 0.05, we reject the null hypothesis that the median of optimality gap is the same for all groups. \n",
        "## Hence, We don’t have sufficient proof to reject that the different types of methods used to lead to statistically significant differences in the acuracy of methods.)"
      ],
      "metadata": {
        "id": "j24bhY-A-AK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruebas de Normalidad de todos los métodos"
      ],
      "metadata": {
        "id": "f5bi4XAqdxjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conduct the  Shapiro-Wilk Test\n",
        "## https://www.geeksforgeeks.org/how-to-perform-a-shapiro-wilk-test-in-python/\n",
        "## This is a hypotheses test and the two hypotheses are as follows:\n",
        "## Ho(Accepted): Sample is from the normal distributions.(Po>0.05)\n",
        "## Ha(Rejected): Sample is not from the normal distributions.\n",
        "## Example 1: Shapiro-Wilk test on the normally distributed sample in Python\n",
        "## In this example, we will be simply using the shapiro() function from \n",
        "## the scipy.stats library to Conduct a Shapiro-Wilk test on the randomly generated data with 500 data points in python. \n",
        "\n",
        "samples = (harjk,harduc,milpc,lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h, lbc1,lbc2,lbc3,lbc4,ks,milp,milp2,)\n",
        "labels  = ['Harjk','Harduc','MILPc','LB1','LB2','LB3','LB4','KS','MILP','MILP2','LB1_1h','LB2_1h','LB3_1h','LB4_1h','KS_1h','MILP_1h','MILP2_1h']\n",
        "normal  = []\n",
        "i       = 0\n",
        "test    = 'Shapiro-Wilk'\n",
        "print('Normality hypothesis test summary ('+filter+')')\n",
        "print('Null hypothesis,Test,Significance,Decision')\n",
        "for s in (samples):\n",
        "  Ho = labels[i]+' '+ 'is from the normal distributions.'\n",
        "  # print(labels[i],':', shapiro(s).statistic, shapiro(s).pvalue)\n",
        "  if shapiro(s).pvalue > 0.05:\n",
        "    decision='We fail to reject the Ho; Ho accepted: samples is from the normal distributions.'\n",
        "    print(labels[i]+' is from the normal distribution,',test,',', round(shapiro(s).pvalue,4),',',decision)\n",
        "    # print(labels[i],'\\t:',round(shapiro(s).statistic,4),'\\t',round(shapiro(s).pvalue,4),'\\tWe fail to reject the null hypothesis; Ho accepted: \\n\\tSample is from the normal distributions.(Po>0.05)')\n",
        "    normal.append(True)\n",
        "  else:\n",
        "    decision='We reject the null hypothesis Ho and accept Ha: samples is not from the normal distributions.'\n",
        "    print(labels[i]+' is from the normal distribution,',test,',', round(shapiro(s).pvalue,4),',',decision)\n",
        "    # print(labels[i],'\\t:',round(shapiro(s).statistic,4),'\\t',round(shapiro(s).pvalue,4),'\\tWe reject the null hypothesis and accept alternative hypothesis Ha: \\n\\tSample is not from the normal distributions.')\n",
        "    normal.append(False)\n",
        "  i = i + 1\n",
        "\n",
        "print('*  Significance level 0.05')\n",
        "print('** Significance level 0.01')\n",
        "## Output Interpretation:\n",
        "## Returns: (statistic: The test statistic, p-value: The p-value for the hypothesis test)\n",
        "## a)Since in the above example, the p-value is 0.73 which is more than the threshold(0.05) which is the alpha(0.05) then we fail to reject the null hypothesis \n",
        "##   i.e. we do (not have sufficient evidence) to say that sample does not come from a normal distribution.\n",
        "## b)Since in the above example, the p-value is 0.0001 which is less than the alpha(0.05) then we reject the null hypothesis \n",
        "##   i.e. we (have sufficient evidence) to say that sample does not come from a normal distribution."
      ],
      "metadata": {
        "id": "7lXF220lEnKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estadistica inferencial para los métodos contructivos"
      ],
      "metadata": {
        "id": "PEm4AYwgau0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Ho(Accepted): The mean for each population is equal.\n",
        "## Ha(Rejected): At least one population mean is different from the rest.\n",
        "## We conduct the ANOVA or Kruskal-wallis test\n",
        "# ==============================================================================\n",
        "labels = ['Harduc','Harjk','MILPc']\n",
        "\n",
        "print('Analysis of variance summary fo constrictive methods')\n",
        "print('Null hypothesis,Test,Significance,Decision')\n",
        "Ho = 'Harduc','Harjk','MILPc'+': The mean for each population is equal.'\n",
        "\n",
        "if normal[0]==True and normal[1]==True and normal[2]==True:\n",
        "  test='ANOVA one way'\n",
        "  #print(stats.f_oneway(milp,lbc1,lbc2,lbc3,lbc4,ks))\n",
        "  if stats.f_oneway(harduc,harjk,milpc).pvalue > 0.05:\n",
        "      decision='We fail to reject the Ho; Ho accepted.'\n",
        "      print(Ho,test,',', round(stats.f_oneway(harduc,harjk,milpc).pvalue,4),',',decision)\n",
        "      # print('[All heuristics]:','\\tWe fail to reject the null hypothesis; Ho accepted:\\n\\tThe mean for each population is equal.')\n",
        "  else:\n",
        "      decision='We reject the Ho and accept Ha: at least one population mean is different from the rest.'\n",
        "      print(Ho,test,',', round(stats.f_oneway(harduc,harjk,milpc).pvalue,4),',',decision)\n",
        "      # print('[All heuristics]:','\\tWe reject the null hypothesis and accept alternative hypothesis Ha:\\n\\tAt least one population mean is different from the rest.')\n",
        "else:\n",
        "  test='Kruskal-wallis test'\n",
        "  #print(stats.kruskal(milp,lbc1,lbc2,lbc3,lbc4,ks))\n",
        "  if stats.kruskal(harduc,harjk,milpc).pvalue > 0.05:\n",
        "      decision='We fail to reject the Ho; Ho accepted.'\n",
        "      print(Ho,test,',', round(stats.kruskal(harduc,harjk,milpc).pvalue,4),',',decision)\n",
        "      # print('[All heuristics]:','\\tWe fail to reject the null hypothesis; Ho accepted:\\n\\tThe mean for each population is equal.')\n",
        "  else:\n",
        "      decision='We reject the Ho and accept Ha: at least one population mean is different from the rest.'\n",
        "      print(Ho,test,',', round(stats.kruskal(harduc,harjk,milpc).pvalue,4),',',decision)\n",
        "print('*  Significance level 0.05')\n",
        "print('** Significance level 0.01')\n",
        "print('\\n')\n",
        "## Output Interpretation:\n",
        "## Null Hypothesis: There is no significant difference between the given conditions of measurement OR the probability \n",
        "## distributions for all the conditions are the same. (Medians are same)\n",
        "## In this example, the test statistic is 13.3514 and the corresponding p-value is p = 0.00126. \n",
        "## Since this p-value is less than 0.05, we can reject the null hypothesis that the mean response time is the same for all three drugs.\n",
        "## In other words, we have sufficient evidence to conclude that the type of drug used leads to statistically significant differences in response time."
      ],
      "metadata": {
        "id": "Gw2sINTSiodh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test of differences between Harduc and Harjk means  \n"
      ],
      "metadata": {
        "id": "h9t2Qzg3fPUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "harducx  = harduc[~np.isnan(harduc)]    ## limpiamos NaN\n",
        "harjkx  = harjk[~np.isnan(harjk)]    ## limpiamos NaN\n",
        "samples = (harducx,harjkx)\n",
        "print('differences means test summary between Harduc and Harjk ('+filter+')')\n",
        "print('Null hypothesis,Test,Significance,Decision')\n",
        "\n",
        "labels = ['Harduc','Harjk']\n",
        "means_test('less',labels,samples,alpha=0.05)\n",
        "\n",
        "print('*  Significance level 0.05')\n",
        "print('** Significance level 0.01')\n",
        "print('\\n')\n",
        "\n",
        "## Output Interpretation:\n",
        "## In the above example, the p-value is 0.2 which is less than the threshold(0.05) which is the alpha(0.05) \n",
        "## i.e. p-value<alpha which means the sample is of the same distribution \n",
        "## and the sample distributions are equal if in the case if the p-value>0.05 than it would be opposite."
      ],
      "metadata": {
        "id": "_97b_pAJ-93K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 HORAS** (ANàlisis de varianza)\n",
        "\n",
        "\n",
        "https://www.geeksforgeeks.org/how-to-perform-a-kruskal-wallis-test-in-python/"
      ],
      "metadata": {
        "id": "e3Pe3UycdeDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1 HORA\n",
        "##labels = ['milp_1h','LB1','LB2','LB3','LB4','ks_1h']\n",
        "\n",
        "print('Analysis of variance summary ('+filter+')'+'(1 hour)')\n",
        "print('Null hypothesis,Test,Significance,Decision')\n",
        "Ho = 'LB1_1h LB2_1h LB3_1h LB4_1h KS_1h MILP_1h MILP2_1h'+': The mean for each population is equal.'\n",
        "\n",
        "if normal[3]==True and normal[4]==True and normal[5]==True and normal[6]==True and normal[7]==True and normal[8]==True and normal[9]==True:\n",
        "  test='ANOVA one way'\n",
        "  #print(stats.f_oneway(lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h))\n",
        "  if stats.f_oneway(lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h).pvalue > 0.05:\n",
        "      decision='We fail to reject the Ho; Ho accepted.'\n",
        "      print(Ho,test,',', round(stats.f_oneway(lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h).pvalue,4),',',decision)\n",
        "      # print('[All heuristics]:','\\tWe fail to reject the null hypothesis; Ho accepted:\\n\\tThe mean for each population is equal.')\n",
        "  else:\n",
        "      decision='We reject the Ho and accept Ha: at least one population mean is different from the rest.'\n",
        "      print(Ho,test,',', round(stats.f_oneway(lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h).pvalue,4),',',decision)\n",
        "      # print('[All heuristics]:','\\tWe reject the null hypothesis and accept alternative hypothesis Ha:\\n\\tAt least one population mean is different from the rest.')\n",
        "else:\n",
        "  test='Kruskal-wallis test'\n",
        "  #print(stats.kruskal(lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h))\n",
        "  if stats.kruskal(lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h).pvalue > 0.05:\n",
        "      decision='We fail to reject the Ho; Ho accepted.'\n",
        "      print(Ho,test,',', round(stats.kruskal(lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h).pvalue,4),',',decision)\n",
        "      # print('[All heuristics]:','\\tWe fail to reject the null hypothesis; Ho accepted:\\n\\tThe mean for each population is equal.')\n",
        "  else:\n",
        "      decision='We reject the Ho and accept Ha: at least one population mean is different from the rest.'\n",
        "      print(Ho,test,',', round(stats.kruskal(lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h).pvalue,4),',',decision)\n",
        "print('*  Significance level 0.05')\n",
        "print('** Significance level 0.01')\n",
        "print('\\n')\n",
        "## Output Interpretation:\n",
        "## Null Hypothesis: There is no significant difference between the given conditions of measurement OR the probability \n",
        "## distributions for all the conditions are the same. (Medians are same)\n",
        "## In this example, the test statistic is 13.3514 and the corresponding p-value is p = 0.00126. \n",
        "## Since this p-value is less than 0.05, we can reject the null hypothesis that the mean response time is the same for all three drugs.\n",
        "## In other words, we have sufficient evidence to conclude that the type of drug used leads to statistically significant differences in response time."
      ],
      "metadata": {
        "id": "I7uxE2NGdULf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of variance between all methods using ANOVA or Kruskal-wallis\n",
        "**2 HORAS**\n",
        "\n",
        "https://www.geeksforgeeks.org/how-to-perform-a-kruskal-wallis-test-in-python/"
      ],
      "metadata": {
        "id": "KF7loWtrU2rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Ho(Accepted): The mean for each population is equal.\n",
        "## Ha(Rejected): At least one population mean is different from the rest.\n",
        "## We conduct the ANOVA or Kruskal-wallis test\n",
        "# ==============================================================================\n",
        "labels = ['LB1','LB2','LB3','LB4','KS','MILP','MILP2']\n",
        "\n",
        "print('Analysis of variance summary ('+filter+')'+'(2 hours)')\n",
        "print('Null hypothesis,Test,Significance,Decision')\n",
        "Ho = 'LB1 LB2 LB3 LB4 KS MILP MILP2'+': The mean for each population is equal.'\n",
        "\n",
        "if normal[10]==True and normal[11]==True and normal[12]==True and normal[13]==True and normal[14]==True and normal[15]==True and normal[16]==True:\n",
        "  test='ANOVA one way'\n",
        "  #print(stats.f_oneway(milp,lbc1,lbc2,lbc3,lbc4,ks))\n",
        "  if stats.f_oneway(lbc1,lbc2,lbc3,lbc4,ks,milp,milp2).pvalue > 0.05:\n",
        "      decision='We fail to reject the Ho; Ho accepted.'\n",
        "      print(Ho,test,',', round(stats.f_oneway(milp,lbc1,lbc2,lbc3,lbc4,ks).pvalue,4),',',decision)\n",
        "      # print('[All heuristics]:','\\tWe fail to reject the null hypothesis; Ho accepted:\\n\\tThe mean for each population is equal.')\n",
        "  else:\n",
        "      decision='We reject the Ho and accept Ha: at least one population mean is different from the rest.'\n",
        "      print(Ho,test,',', round(stats.f_oneway(lbc1,lbc2,lbc3,lbc4,ks,milp,milp2).pvalue,4),',',decision)\n",
        "      # print('[All heuristics]:','\\tWe reject the null hypothesis and accept alternative hypothesis Ha:\\n\\tAt least one population mean is different from the rest.')\n",
        "else:\n",
        "  test='Kruskal-wallis test'\n",
        "  #print(stats.kruskal(milp,lbc1,lbc2,lbc3,lbc4,ks))\n",
        "  if stats.kruskal(lbc1,lbc2,lbc3,lbc4,ks,milp,milp2).pvalue > 0.05:\n",
        "      decision='We fail to reject the Ho; Ho accepted.'\n",
        "      print(Ho,test,',', round(stats.kruskal(lbc1,lbc2,lbc3,lbc4,ks,milp,milp2).pvalue,4),',',decision)\n",
        "      # print('[All heuristics]:','\\tWe fail to reject the null hypothesis; Ho accepted:\\n\\tThe mean for each population is equal.')\n",
        "  else:\n",
        "      decision='We reject the Ho and accept Ha: at least one population mean is different from the rest.'\n",
        "      print(Ho,test,',', round(stats.kruskal(lbc1,lbc2,lbc3,lbc4,ks,milp,milp2).pvalue,4),',',decision)\n",
        "print('*  Significance level 0.05')\n",
        "print('** Significance level 0.01')\n",
        "print('\\n')\n",
        "## Output Interpretation:\n",
        "## Null Hypothesis: There is no significant difference between the given conditions of measurement OR the probability \n",
        "## distributions for all the conditions are the same. (Medians are same)\n",
        "## In this example, the test statistic is 13.3514 and the corresponding p-value is p = 0.00126. \n",
        "## Since this p-value is less than 0.05, we can reject the null hypothesis that the mean response time is the same for all three drugs.\n",
        "## In other words, we have sufficient evidence to conclude that the type of drug used leads to statistically significant differences in response time."
      ],
      "metadata": {
        "id": "jIDlKZerVPc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Hoc analysis"
      ],
      "metadata": {
        "id": "KlYZGEgF4FQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ONE HOURS\n",
        "## We applying the Post Hoc analysis with Mann-whitney U Test to know which pairs have a significant difference between them.\n",
        "## The Mann–Whitney U test is applied to independent samples.\n",
        "# ==============================================================================\n",
        "## We conduct the Mann-whitney U {“two-sided”, “greater”, “less”}, optional\n",
        "# ==============================================================================\n",
        "samples= [ lbc1_1h,lbc2_1h,lbc3_1h,lbc4_1h,ks_1h,milp_1h,milp2_1h ]\n",
        "labels = ['LB1_1h','LB2_1h','LB3_1h','LB4_1h','KS_1h','MILP_1h','MILP2_1h']\n",
        "means  = []\n",
        "print('\\n RANKING (1 HOUR)==============================================================================\\n')\n",
        "i = 0\n",
        "ordered_labels  = []\n",
        "ordered_samples = []\n",
        "for s in samples:\n",
        "  means.append((i,np.mean(s)))\n",
        "  i = i + 1\n",
        "means.sort(key=lambda tup: tup[1], reverse=False) ##reverse=False\n",
        "i = 0\n",
        "for m in means:  \n",
        "  ordered_samples.append(samples[m[0]])\n",
        "  ordered_labels.append(labels[m[0]])\n",
        "  i = i + 1\n",
        "  print(labels[m[0]],m[1])\n",
        "print('\\n Ha:less,alpha=0.05 (1 HOUR)=====================================================================\\n')\n",
        "print('Means difference hypothesis test summary ('+filter+')')\n",
        "print('Null hypothesis,Test,Significance,Decision')\n",
        "for i in range(len(ordered_labels)):\n",
        "  for j in range(len(ordered_labels)):\n",
        "    if j > i:\n",
        "      means_test('less',(ordered_labels[i],ordered_labels[j]),(ordered_samples[i],ordered_samples[j]),alpha=0.05)\n",
        "print('\\n Ha:less,alpha=0.01 (1 HOUR)=====================================================================\\n')\n",
        "for i in range(len(ordered_labels)):\n",
        "  for j in range(len(ordered_labels)):\n",
        "    if j > i:\n",
        "      means_test('less',(ordered_labels[i],ordered_labels[j]),(ordered_samples[i],ordered_samples[j]),alpha=0.01)"
      ],
      "metadata": {
        "id": "WR4M0BAbfuD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TWO HOURS\n",
        "## We applying the Post Hoc analysis with Mann-whitney U Test to know which pairs have a significant difference between them.\n",
        "## The Mann–Whitney U test is applied to independent samples.\n",
        "# ==============================================================================\n",
        "## We conduct the Mann-whitney U {“two-sided”, “greater”, “less”}, optional\n",
        "# ==============================================================================\n",
        "samples= [ lbc1,lbc2,lbc3,lbc4,ks,milp,milp2 ]\n",
        "labels = ['LB1','LB2','LB3','LB4','KS','MILP','MILP2']\n",
        "means  = []\n",
        "print('\\n RANKING (2 HOUR)==============================================================================\\n')\n",
        "i = 0\n",
        "ordered_labels  = []\n",
        "ordered_samples = []\n",
        "for s in samples:\n",
        "  means.append((i,np.mean(s)))\n",
        "  i = i + 1\n",
        "means.sort(key=lambda tup: tup[1], reverse=False) ##reverse=False\n",
        "i = 0\n",
        "for m in means:  \n",
        "  ordered_samples.append(samples[m[0]])\n",
        "  ordered_labels.append(labels[m[0]])\n",
        "  i = i + 1\n",
        "  print(labels[m[0]],m[1])\n",
        "print('\\n Ha:less,alpha=0.05 (2 HOUR)======================================================================\\n')\n",
        "print('Means difference hypothesis test summary ('+filter+')')\n",
        "print('Null hypothesis,Test,Significance,Decision')\n",
        "for i in range(len(ordered_labels)):\n",
        "  for j in range(len(ordered_labels)):\n",
        "    if j > i:\n",
        "      means_test('less',(ordered_labels[i],ordered_labels[j]),(ordered_samples[i],ordered_samples[j]),alpha=0.05)\n",
        "print('\\n Ha:less,alpha=0.01 (2 HOUR)=======================================================================\\n')\n",
        "for i in range(len(ordered_labels)):\n",
        "  for j in range(len(ordered_labels)):\n",
        "    if j > i:\n",
        "      means_test('less',(ordered_labels[i],ordered_labels[j]),(ordered_samples[i],ordered_samples[j]),alpha=0.01)"
      ],
      "metadata": {
        "id": "KA0g_tnl3-jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descartes"
      ],
      "metadata": {
        "id": "-bObL_YW-7P9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruebas de Homocedasticidad entre los resultados de Harduc y Harjk"
      ],
      "metadata": {
        "id": "XPVgaGKQZMfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## homocedasticidad.\n",
        "## https://www.cienciadedatos.net/documentos/pystats07-test-homocedasticidad-heterocedasticidad-python.html\n",
        "## https://stats.stackexchange.com/questions/135232/bartletts-test-vs-levenes-test\n",
        "## We conducting Levene’s Test or Bartlett’s Test of homogeneity of variance we are dealing with two hypotheses. These two are simply put:\n",
        "## Ho(Accepted): the variances are equal across all samples\n",
        "## Ha(Rejected): the variances are not equal across all samples\n",
        "\n",
        "test    = 'Levene'\n",
        "print('Homoscedasticity hypothesis test summary ('+filter+')')\n",
        "print('Null hypothesis,Test,Significance,Decision')\n",
        "Ho = 'Harduc and Harjk'+': the variances are equal.'\n",
        "\n",
        "# Levene test\n",
        "# ==============================================================================\n",
        "levene_test = stats.levene(harjk, harduc, center='mean')\n",
        "# print(levene_test)\n",
        "equalvar=False\n",
        "if levene_test.pvalue > 0.05:\n",
        "    decision='We fail to reject the Ho; Ho accepted.'\n",
        "    print('Harjk,Harduc'+' the variances are equal across all samples,',test,',', round(levene_test.pvalue,4),',',decision)\n",
        "    # print('[Harjk,Harduc]\\t:',round(levene_test.statistic,4),round(levene_test.pvalue,4),'\\tWe fail to reject the null hypothesis; Ho accepted: the variances are equal across all samples. (Po>0.05)')\n",
        "    equalvar=True\n",
        "else:\n",
        "    decision='We reject the Ho and accept Ha: the variances are not equal.'\n",
        "    print('Harjk,Harduc'+' the variances are equal across all samples,',test,',', round(levene_test.pvalue,4),',',decision)\n",
        "    # print('[Harjk,Harduc]\\t:',round(levene_test.statistic,4),round(levene_test.pvalue,4),'\\tWe reject the null hypothesis and accept Ha: the variances are not equal across all samples. (Po<=0.05)')\n",
        "print('*  Significance level 0.05')\n",
        "print('** Significance level 0.01')\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# Fligner-Killeen\n",
        "# ==============================================================================\n",
        "fligner_test = stats.fligner(harjk, harduc, center='mean')\n",
        "print(fligner_test)\n",
        "if fligner_test.pvalue > 0.05:\n",
        "    print('[Harjk,Harduc]\\t:',round(fligner_test.statistic,4),round(fligner_test.pvalue,4),'\\tWe fail to reject the null hypothesis; Ho accepted: the variances are equal across all samples. (Po>0.05)')\n",
        "else:\n",
        "    print('[Harjk,Harduc]\\t:',round(fligner_test.statistic,4),round(fligner_test.pvalue,4),'\\tWe reject the null hypothesis and accept Ha: the variances are not equal across all samples. (Po<=0.05)')\n",
        "\n",
        "## Output Interpretation:\n",
        "## a) This means, for example, that if we get a p-value larger than 0.05 we can assume \n",
        "## hat our data is heteroscedastic and we can continue carrying out a parametric test \n",
        "## such as the two-sample t-test. \n",
        "## b) If we, on the other hand, get a statistically \n",
        "## significant result we may want to carry out the Mann-Whitney U test. \n",
        "## Ninguno de los test muestra evidencias para rechazar la hipótesis de que los \n",
        "## dos grupos tienen la misma varianza, homocedasticidad. p-value >>>> 0.05"
      ],
      "metadata": {
        "id": "2nBlWyV6I8an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conduct the Kruskal-Wallis Test\n",
        "result = stats.kruskal(milp,lbc1,lbc2,lbc3,lbc4,ks)\n",
        "\n",
        "# Print the result\n",
        "print(result)\n",
        "#Kruskal-Wallis test discussion:  In this example, the test statistic comes out to be equal to 87 and the corresponding p-value is 2.1856E-17. \n",
        "#(As the p-value is not less than 0.05, we cannot reject the null hypothesis that the median of optimality gap is the same for all groups. Hence, We don’t have sufficient proof to claim that the different types of methods used to lead to statistically significant differences in the acuracy of methods.)\n",
        "#(As the p-value is less than 0.05, we reject the null hypothesis that the median of optimality gap is the same for all groups. Hence, We don’t have sufficient proof to reject that the different types of methods used to lead to statistically significant differences in the acuracy of methods.)"
      ],
      "metadata": {
        "id": "adpMXMMSGO-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## https://www.geeksforgeeks.org/data-visualization-with-python-seaborn/\n",
        "if False:\n",
        "  nor  = np.random.normal(size=100) \n",
        "  nor  = nor + 5\n",
        "  ale  = np.random.uniform(size=100) \n",
        "  nor1 = np.random.normal(size=100) \n",
        "  nor1  = nor1 + 5\n",
        "  ale1 = np.random.uniform(size=100)\n",
        "  ale1  = ale1 + 5\n",
        "  nor2 = np.random.normal(size=100)\n",
        "  nor2  = nor2 + 5\n",
        "  ale2 = np.random.uniform(size=100)  #'N','A','N1','A1','N2','A2',,nor,ale,nor1,ale1,nor2,ale2\n",
        "  ale2  = ale2 + 5\n",
        "  fig, ax = plt.subplots(figsize=(8, 6))\n",
        "  method1 = ['nor' ]*(len(nor))\n",
        "  method2 = ['ale' ]*(len(ale))\n",
        "  method3 = ['nor1']*(len(nor1))\n",
        "  method4 = ['ale1']*(len(ale1))\n",
        "  method5 = ['nor2']*(len(nor2))\n",
        "  method6 = ['ale2']*(len(ale2))\n",
        "  method  = method1 + method2 + method3 + method4 + method5 + method6\n",
        "  data    = np.concatenate((nor,ale,nor1,ale1,nor2,ale2), axis=0)\n",
        "  # initialise data of lists.\n",
        "  data = {'gap'       :data,\n",
        "          'method'  :method}  \n",
        "  df1 = pd.DataFrame( data )\n",
        "  sns.histplot(df1, x=\"gap\", hue=\"method\", element=\"step\") #,bins=15\n",
        "\n",
        "  print('Mann-whitney U test NOR-ALE')\n",
        "  if stats.mannwhitneyu(ale, nor, alternative='less').pvalue > 0.05:\n",
        "      print('[ale,nor]\\t:','We fail to reject the null hypothesis; Ho accepted: The median difference between the pairs follows a symmetric distribution around zero.')\n",
        "  else:\n",
        "      print('[ale,nor]\\t:','We reject the null hypothesis and accept Ha: The median difference is positive α=0.05. Ale\\'s mean is lower than Nor\\'s mean.')\n",
        "\n",
        "  print('kruskal-wallis test nor,ale1,nor1,ale2,nor2') ## ale,\n",
        "  if stats.kruskal(ale,nor,nor1,nor2).pvalue > 0.05:\n",
        "      print('[All heuristics]\\t:','We fail to reject the null hypothesis; Ho accepted: The mean for each population is equal.')\n",
        "  else:\n",
        "      print('[All heuristics]\\t:','We reject the null hypothesis and accept Ha: At least one population mean is different from the rest: α=0.05.')\n"
      ],
      "metadata": {
        "id": "3114uAwrclTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wilcoxon test\n",
        "https://www.geeksforgeeks.org/how-to-conduct-a-wilcoxon-signed-rank-test-in-python/\n",
        "\n",
        "https://pythonfordatascienceorg.wordpress.com/wilcoxon-sign-ranked-test-python/\n",
        "\n",
        "https://www.cienciadedatos.net/documentos/18_prueba_de_los_rangos_con_signo_de_wilcoxon\n",
        "\n",
        "Comparaxion medias\n",
        "\n",
        "https://www.cienciadedatos.net/documentos/pystats11-wilcoxon-mann-whitney-u-test-python\n",
        "\n",
        "https://www.cienciadedatos.net/documentos/pystats10-t-test-python.html\n",
        "\n",
        "https://www.cienciadedatos.net/documentos/pystats09-analisis-de-varianza-anova-python.html\n",
        "\n",
        "https://www.cienciadedatos.net/documentos/15_inferencia_para_proporciones"
      ],
      "metadata": {
        "id": "CjAQWTIsAtHA"
      }
    }
  ]
}